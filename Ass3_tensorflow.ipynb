{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ass3_tensorflow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2UjXBeMgbip"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqt_iw-O0S_H"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def normalize_sentence(s):\n",
        "    s = unicode_to_ascii(s.strip())\n",
        "    new_s = re.findall(r\"[\\w']+|[.,!?;\\-%]\", s)\n",
        "    normal_s = []\n",
        "    for word in new_s:\n",
        "#      if not word.lower() == word:\n",
        "#        normal_s.append(\"<cap>\")\n",
        "      normal_s.append(word.lower())\n",
        "    return '<start> ' + ' '.join(normal_s) + ' <end>'\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKSAMg7pHyyT"
      },
      "source": [
        "sanity check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOML5g3Y1Eqt",
        "outputId": "29241d93-14d2-4d50-d810-818e88c6340f"
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "ru_sentence = u\"Я могу одолжить эту книгу?\"\n",
        "print(normalize_sentence(en_sentence))\n",
        "print(normalize_sentence(ru_sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "<start> я могу одолжить эту книгу ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar8OXtaD-fGO"
      },
      "source": [
        "def create_dataset(num_examples):\n",
        "  print(\"Opening files...\")   \n",
        "  en_lines = open('corpora/corpus.en_ru.1m.en', encoding='utf-8').read().strip().split('\\n')\n",
        "  print(\"Opened English file.\")\n",
        "  ru_lines = open('corpora/corpus.en_ru.1m.ru', encoding='utf-8').read().strip().split('\\n')\n",
        "  print(\"Opened Russian file.\")\n",
        "  pairs = [(normalize_sentence(ru_lines[i]), normalize_sentence(en_lines[i])) \n",
        "          for i in range(num_examples)]\n",
        "  print(\"Prepared %s sentence pairs\" % len(pairs))\n",
        "  return zip(*pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2lt8bc9Hw-P"
      },
      "source": [
        "sanity check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsOP3CJ9B9C-",
        "outputId": "628f5ae6-e923-4a86-8c0a-e3ad5a4d7d86"
      },
      "source": [
        "en, ru = create_dataset(10000)\n",
        "print(en[-1])\n",
        "print(ru[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Opening files...\n",
            "Opened English file.\n",
            "Opened Russian file.\n",
            "Prepared 10000 sentence pairs\n",
            "<start> не следует ли предоставить пкрк право деиствовать посредником государств - членов при возникновении у них сомнении относительно тех или иных аспектов деятельности воз в их регионе или в более широком плане ? <end>\n",
            "<start> should the scrc have a remit to act as a conduit for member states who have concerns with the way in which who is performing in their region or more generally ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ9CcSAZYM1P"
      },
      "source": [
        "alternate solution begin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j440xzwZYDQS",
        "outputId": "8cb08149-9816-4790-8160-f00c964d282b"
      },
      "source": [
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bI4L3PoYRAJ"
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def n_create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DbPochkGwNg"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6T7MEpOHAIk"
      },
      "source": [
        "def load_dataset(path, num_examples):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = n_create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBW87VxjHJQ6",
        "outputId": "b1c21c79-e13e-479e-9669-01d16c437602"
      },
      "source": [
        "num_examples = 30000\n",
        "\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "print(max_length_targ)\n",
        "print(max_length_inp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n",
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6btOKMslHejy",
        "outputId": "2cef6f09-8afa-4614-eb0f-df40aaf1afc4"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24000 24000 6000 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSlfWFzkHk19"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfFSt2UaHuP2"
      },
      "source": [
        "sanity check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfcrEsTXHmv4",
        "outputId": "79d7e971-5871-47b2-a391-506cb8fa71bb"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "17 ----> se\n",
            "468 ----> sento\n",
            "20 ----> en\n",
            "9 ----> el\n",
            "594 ----> banco\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "27 ----> she\n",
            "321 ----> sat\n",
            "44 ----> on\n",
            "13 ----> the\n",
            "578 ----> bank\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMSYVKTgH0mK"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BStPv7LRH524",
        "outputId": "f291e059-e4df-4cf3-e903-32b146d84d71"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 11]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-waugKxH-HV"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-4J53n4H_Gs",
        "outputId": "c397f0c4-89a4-484e-9de6-e89797990c51"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lFkJHNOIFC5"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcsarSEPIHTt",
        "outputId": "94c02404-f21c-40eb-cd72-e9fa0c27fc39"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3Hbc868ILK2"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAocLRXaINul",
        "outputId": "68d5829b-eee0-4aee-dd25-fa969a84c0b6"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 4935)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMulIwxIISsa"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjdAvVDrIVaR"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5RL41xEIcUQ"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Eeaqd2UIgGt",
        "outputId": "27021179-1784-4b4f-b1d2-ca270c0c7cb4"
      },
      "source": [
        "EPOCHS = 10\n",
        "print(\"Steps per epoch = %s\" % steps_per_epoch)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    # print('Started Epoch {} Batch {}'.format(epoch + 1, batch + 1))\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 10 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps per epoch = 375\n",
            "Epoch 1 Batch 0 Loss 4.6510\n",
            "Epoch 1 Batch 10 Loss 3.0085\n",
            "Epoch 1 Batch 20 Loss 2.8721\n",
            "Epoch 1 Batch 30 Loss 2.5524\n",
            "Epoch 1 Batch 40 Loss 2.5252\n",
            "Epoch 1 Batch 50 Loss 2.2358\n",
            "Epoch 1 Batch 60 Loss 2.2888\n",
            "Epoch 1 Batch 70 Loss 2.2517\n",
            "Epoch 1 Batch 80 Loss 2.2459\n",
            "Epoch 1 Batch 90 Loss 2.0716\n",
            "Epoch 1 Batch 100 Loss 2.1007\n",
            "Epoch 1 Batch 110 Loss 2.1301\n",
            "Epoch 1 Batch 120 Loss 2.1795\n",
            "Epoch 1 Batch 130 Loss 2.0928\n",
            "Epoch 1 Batch 140 Loss 1.9857\n",
            "Epoch 1 Batch 150 Loss 1.9579\n",
            "Epoch 1 Batch 160 Loss 1.9235\n",
            "Epoch 1 Batch 170 Loss 1.9746\n",
            "Epoch 1 Batch 180 Loss 1.9062\n",
            "Epoch 1 Batch 190 Loss 1.8951\n",
            "Epoch 1 Batch 200 Loss 2.0131\n",
            "Epoch 1 Batch 210 Loss 1.9178\n",
            "Epoch 1 Batch 220 Loss 1.9138\n",
            "Epoch 1 Batch 230 Loss 1.9187\n",
            "Epoch 1 Batch 240 Loss 1.9462\n",
            "Epoch 1 Batch 250 Loss 1.7492\n",
            "Epoch 1 Batch 260 Loss 1.8520\n",
            "Epoch 1 Batch 270 Loss 1.8834\n",
            "Epoch 1 Batch 280 Loss 1.7663\n",
            "Epoch 1 Batch 290 Loss 1.8118\n",
            "Epoch 1 Batch 300 Loss 1.7609\n",
            "Epoch 1 Batch 310 Loss 1.8197\n",
            "Epoch 1 Batch 320 Loss 1.7095\n",
            "Epoch 1 Batch 330 Loss 1.7469\n",
            "Epoch 1 Batch 340 Loss 1.7862\n",
            "Epoch 1 Batch 350 Loss 1.7794\n",
            "Epoch 1 Batch 360 Loss 1.6427\n",
            "Epoch 1 Batch 370 Loss 1.7286\n",
            "Epoch 1 Loss 2.0619\n",
            "Time taken for 1 epoch 1749.855524301529 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.5964\n",
            "Epoch 2 Batch 10 Loss 1.8040\n",
            "Epoch 2 Batch 20 Loss 1.6893\n",
            "Epoch 2 Batch 30 Loss 1.6661\n",
            "Epoch 2 Batch 40 Loss 1.7163\n",
            "Epoch 2 Batch 50 Loss 1.5824\n",
            "Epoch 2 Batch 60 Loss 1.5481\n",
            "Epoch 2 Batch 70 Loss 1.6113\n",
            "Epoch 2 Batch 80 Loss 1.6678\n",
            "Epoch 2 Batch 90 Loss 1.6073\n",
            "Epoch 2 Batch 100 Loss 1.4349\n",
            "Epoch 2 Batch 110 Loss 1.5497\n",
            "Epoch 2 Batch 120 Loss 1.5943\n",
            "Epoch 2 Batch 130 Loss 1.5790\n",
            "Epoch 2 Batch 140 Loss 1.5344\n",
            "Epoch 2 Batch 150 Loss 1.3796\n",
            "Epoch 2 Batch 160 Loss 1.5032\n",
            "Epoch 2 Batch 170 Loss 1.4737\n",
            "Epoch 2 Batch 180 Loss 1.5640\n",
            "Epoch 2 Batch 190 Loss 1.5573\n",
            "Epoch 2 Batch 200 Loss 1.4038\n",
            "Epoch 2 Batch 210 Loss 1.5600\n",
            "Epoch 2 Batch 220 Loss 1.5363\n",
            "Epoch 2 Batch 230 Loss 1.4268\n",
            "Epoch 2 Batch 240 Loss 1.4338\n",
            "Epoch 2 Batch 250 Loss 1.4615\n",
            "Epoch 2 Batch 260 Loss 1.3171\n",
            "Epoch 2 Batch 270 Loss 1.3477\n",
            "Epoch 2 Batch 280 Loss 1.4998\n",
            "Epoch 2 Batch 290 Loss 1.4885\n",
            "Epoch 2 Batch 300 Loss 1.3765\n",
            "Epoch 2 Batch 310 Loss 1.5222\n",
            "Epoch 2 Batch 320 Loss 1.3933\n",
            "Epoch 2 Batch 330 Loss 1.4504\n",
            "Epoch 2 Batch 340 Loss 1.5215\n",
            "Epoch 2 Batch 350 Loss 1.3773\n",
            "Epoch 2 Batch 360 Loss 1.3646\n",
            "Epoch 2 Batch 370 Loss 1.2653\n",
            "Epoch 2 Loss 1.5114\n",
            "Time taken for 1 epoch 1740.0771107673645 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.2627\n",
            "Epoch 3 Batch 10 Loss 1.2601\n",
            "Epoch 3 Batch 20 Loss 1.2068\n",
            "Epoch 3 Batch 30 Loss 1.1936\n",
            "Epoch 3 Batch 40 Loss 1.1242\n",
            "Epoch 3 Batch 50 Loss 1.2790\n",
            "Epoch 3 Batch 60 Loss 1.3219\n",
            "Epoch 3 Batch 70 Loss 1.3168\n",
            "Epoch 3 Batch 80 Loss 1.2118\n",
            "Epoch 3 Batch 90 Loss 1.1398\n",
            "Epoch 3 Batch 100 Loss 1.2617\n",
            "Epoch 3 Batch 110 Loss 1.0938\n",
            "Epoch 3 Batch 120 Loss 1.0929\n",
            "Epoch 3 Batch 130 Loss 1.0743\n",
            "Epoch 3 Batch 140 Loss 1.2517\n",
            "Epoch 3 Batch 150 Loss 1.2712\n",
            "Epoch 3 Batch 160 Loss 1.2227\n",
            "Epoch 3 Batch 170 Loss 1.1166\n",
            "Epoch 3 Batch 180 Loss 1.0729\n",
            "Epoch 3 Batch 190 Loss 1.0883\n",
            "Epoch 3 Batch 200 Loss 1.0530\n",
            "Epoch 3 Batch 210 Loss 1.0882\n",
            "Epoch 3 Batch 220 Loss 1.2240\n",
            "Epoch 3 Batch 230 Loss 1.0219\n",
            "Epoch 3 Batch 240 Loss 1.1370\n",
            "Epoch 3 Batch 250 Loss 1.0581\n",
            "Epoch 3 Batch 260 Loss 1.1343\n",
            "Epoch 3 Batch 270 Loss 1.0880\n",
            "Epoch 3 Batch 280 Loss 1.1007\n",
            "Epoch 3 Batch 290 Loss 1.0431\n",
            "Epoch 3 Batch 300 Loss 0.9830\n",
            "Epoch 3 Batch 310 Loss 0.9313\n",
            "Epoch 3 Batch 320 Loss 1.1017\n",
            "Epoch 3 Batch 330 Loss 1.0623\n",
            "Epoch 3 Batch 340 Loss 1.0607\n",
            "Epoch 3 Batch 350 Loss 1.1007\n",
            "Epoch 3 Batch 360 Loss 0.9981\n",
            "Epoch 3 Batch 370 Loss 0.9122\n",
            "Epoch 3 Loss 1.1289\n",
            "Time taken for 1 epoch 1734.107624053955 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.8453\n",
            "Epoch 4 Batch 10 Loss 0.8869\n",
            "Epoch 4 Batch 20 Loss 0.8999\n",
            "Epoch 4 Batch 30 Loss 0.8241\n",
            "Epoch 4 Batch 40 Loss 0.8923\n",
            "Epoch 4 Batch 50 Loss 0.8640\n",
            "Epoch 4 Batch 60 Loss 0.7711\n",
            "Epoch 4 Batch 70 Loss 0.8867\n",
            "Epoch 4 Batch 80 Loss 0.7815\n",
            "Epoch 4 Batch 90 Loss 0.7990\n",
            "Epoch 4 Batch 100 Loss 0.8261\n",
            "Epoch 4 Batch 110 Loss 0.7226\n",
            "Epoch 4 Batch 120 Loss 0.8574\n",
            "Epoch 4 Batch 130 Loss 0.8087\n",
            "Epoch 4 Batch 140 Loss 0.8221\n",
            "Epoch 4 Batch 150 Loss 0.8667\n",
            "Epoch 4 Batch 160 Loss 0.9217\n",
            "Epoch 4 Batch 170 Loss 0.7766\n",
            "Epoch 4 Batch 180 Loss 0.7537\n",
            "Epoch 4 Batch 190 Loss 0.8967\n",
            "Epoch 4 Batch 200 Loss 0.7318\n",
            "Epoch 4 Batch 210 Loss 0.8671\n",
            "Epoch 4 Batch 220 Loss 0.8050\n",
            "Epoch 4 Batch 230 Loss 0.7776\n",
            "Epoch 4 Batch 240 Loss 0.7573\n",
            "Epoch 4 Batch 250 Loss 0.7535\n",
            "Epoch 4 Batch 260 Loss 0.7409\n",
            "Epoch 4 Batch 270 Loss 0.7261\n",
            "Epoch 4 Batch 280 Loss 0.7401\n",
            "Epoch 4 Batch 290 Loss 0.7198\n",
            "Epoch 4 Batch 300 Loss 0.7171\n",
            "Epoch 4 Batch 310 Loss 0.6900\n",
            "Epoch 4 Batch 320 Loss 0.7409\n",
            "Epoch 4 Batch 330 Loss 0.7124\n",
            "Epoch 4 Batch 340 Loss 0.9026\n",
            "Epoch 4 Batch 350 Loss 0.7753\n",
            "Epoch 4 Batch 360 Loss 0.7174\n",
            "Epoch 4 Batch 370 Loss 0.7458\n",
            "Epoch 4 Loss 0.8095\n",
            "Time taken for 1 epoch 1742.3865568637848 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.5890\n",
            "Epoch 5 Batch 10 Loss 0.5712\n",
            "Epoch 5 Batch 20 Loss 0.5929\n",
            "Epoch 5 Batch 30 Loss 0.5635\n",
            "Epoch 5 Batch 40 Loss 0.7439\n",
            "Epoch 5 Batch 50 Loss 0.6904\n",
            "Epoch 5 Batch 60 Loss 0.6254\n",
            "Epoch 5 Batch 70 Loss 0.4892\n",
            "Epoch 5 Batch 80 Loss 0.5916\n",
            "Epoch 5 Batch 90 Loss 0.4713\n",
            "Epoch 5 Batch 100 Loss 0.5585\n",
            "Epoch 5 Batch 110 Loss 0.5526\n",
            "Epoch 5 Batch 120 Loss 0.6084\n",
            "Epoch 5 Batch 130 Loss 0.5109\n",
            "Epoch 5 Batch 140 Loss 0.5949\n",
            "Epoch 5 Batch 150 Loss 0.5588\n",
            "Epoch 5 Batch 160 Loss 0.5129\n",
            "Epoch 5 Batch 170 Loss 0.5457\n",
            "Epoch 5 Batch 180 Loss 0.4781\n",
            "Epoch 5 Batch 190 Loss 0.6429\n",
            "Epoch 5 Batch 200 Loss 0.6207\n",
            "Epoch 5 Batch 210 Loss 0.6023\n",
            "Epoch 5 Batch 220 Loss 0.5349\n",
            "Epoch 5 Batch 230 Loss 0.6071\n",
            "Epoch 5 Batch 240 Loss 0.4964\n",
            "Epoch 5 Batch 250 Loss 0.5142\n",
            "Epoch 5 Batch 260 Loss 0.5131\n",
            "Epoch 5 Batch 270 Loss 0.4892\n",
            "Epoch 5 Batch 280 Loss 0.5525\n",
            "Epoch 5 Batch 290 Loss 0.5252\n",
            "Epoch 5 Batch 300 Loss 0.5562\n",
            "Epoch 5 Batch 310 Loss 0.5569\n",
            "Epoch 5 Batch 320 Loss 0.5743\n",
            "Epoch 5 Batch 330 Loss 0.5481\n",
            "Epoch 5 Batch 340 Loss 0.5898\n",
            "Epoch 5 Batch 350 Loss 0.5252\n",
            "Epoch 5 Batch 360 Loss 0.6123\n",
            "Epoch 5 Batch 370 Loss 0.5499\n",
            "Epoch 5 Loss 0.5712\n",
            "Time taken for 1 epoch 1735.3531699180603 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.3705\n",
            "Epoch 6 Batch 10 Loss 0.3954\n",
            "Epoch 6 Batch 20 Loss 0.4324\n",
            "Epoch 6 Batch 30 Loss 0.4232\n",
            "Epoch 6 Batch 40 Loss 0.4450\n",
            "Epoch 6 Batch 50 Loss 0.4638\n",
            "Epoch 6 Batch 60 Loss 0.4481\n",
            "Epoch 6 Batch 70 Loss 0.3773\n",
            "Epoch 6 Batch 80 Loss 0.3607\n",
            "Epoch 6 Batch 90 Loss 0.3956\n",
            "Epoch 6 Batch 100 Loss 0.3758\n",
            "Epoch 6 Batch 110 Loss 0.4244\n",
            "Epoch 6 Batch 120 Loss 0.3559\n",
            "Epoch 6 Batch 130 Loss 0.3162\n",
            "Epoch 6 Batch 140 Loss 0.3987\n",
            "Epoch 6 Batch 150 Loss 0.4004\n",
            "Epoch 6 Batch 160 Loss 0.4244\n",
            "Epoch 6 Batch 170 Loss 0.3936\n",
            "Epoch 6 Batch 180 Loss 0.3879\n",
            "Epoch 6 Batch 190 Loss 0.4160\n",
            "Epoch 6 Batch 200 Loss 0.4032\n",
            "Epoch 6 Batch 210 Loss 0.4215\n",
            "Epoch 6 Batch 220 Loss 0.4894\n",
            "Epoch 6 Batch 230 Loss 0.4146\n",
            "Epoch 6 Batch 240 Loss 0.4440\n",
            "Epoch 6 Batch 250 Loss 0.4357\n",
            "Epoch 6 Batch 260 Loss 0.4562\n",
            "Epoch 6 Batch 270 Loss 0.4494\n",
            "Epoch 6 Batch 280 Loss 0.4189\n",
            "Epoch 6 Batch 290 Loss 0.3990\n",
            "Epoch 6 Batch 300 Loss 0.4016\n",
            "Epoch 6 Batch 310 Loss 0.4554\n",
            "Epoch 6 Batch 320 Loss 0.3461\n",
            "Epoch 6 Batch 330 Loss 0.5339\n",
            "Epoch 6 Batch 340 Loss 0.4238\n",
            "Epoch 6 Batch 350 Loss 0.3987\n",
            "Epoch 6 Batch 360 Loss 0.4585\n",
            "Epoch 6 Batch 370 Loss 0.3797\n",
            "Epoch 6 Loss 0.4049\n",
            "Time taken for 1 epoch 1753.629796743393 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.2864\n",
            "Epoch 7 Batch 10 Loss 0.2710\n",
            "Epoch 7 Batch 20 Loss 0.2765\n",
            "Epoch 7 Batch 30 Loss 0.2801\n",
            "Epoch 7 Batch 40 Loss 0.2382\n",
            "Epoch 7 Batch 50 Loss 0.2453\n",
            "Epoch 7 Batch 60 Loss 0.2873\n",
            "Epoch 7 Batch 70 Loss 0.3088\n",
            "Epoch 7 Batch 80 Loss 0.3030\n",
            "Epoch 7 Batch 90 Loss 0.3032\n",
            "Epoch 7 Batch 100 Loss 0.2999\n",
            "Epoch 7 Batch 110 Loss 0.2763\n",
            "Epoch 7 Batch 120 Loss 0.2155\n",
            "Epoch 7 Batch 130 Loss 0.2421\n",
            "Epoch 7 Batch 140 Loss 0.3169\n",
            "Epoch 7 Batch 150 Loss 0.2815\n",
            "Epoch 7 Batch 160 Loss 0.3241\n",
            "Epoch 7 Batch 170 Loss 0.3297\n",
            "Epoch 7 Batch 180 Loss 0.3877\n",
            "Epoch 7 Batch 190 Loss 0.2525\n",
            "Epoch 7 Batch 200 Loss 0.3429\n",
            "Epoch 7 Batch 210 Loss 0.2556\n",
            "Epoch 7 Batch 220 Loss 0.3126\n",
            "Epoch 7 Batch 230 Loss 0.3476\n",
            "Epoch 7 Batch 240 Loss 0.3035\n",
            "Epoch 7 Batch 250 Loss 0.2764\n",
            "Epoch 7 Batch 260 Loss 0.2930\n",
            "Epoch 7 Batch 270 Loss 0.3308\n",
            "Epoch 7 Batch 280 Loss 0.3834\n",
            "Epoch 7 Batch 290 Loss 0.3136\n",
            "Epoch 7 Batch 300 Loss 0.3446\n",
            "Epoch 7 Batch 310 Loss 0.3886\n",
            "Epoch 7 Batch 320 Loss 0.3716\n",
            "Epoch 7 Batch 330 Loss 0.2826\n",
            "Epoch 7 Batch 340 Loss 0.2757\n",
            "Epoch 7 Batch 350 Loss 0.3128\n",
            "Epoch 7 Batch 360 Loss 0.2480\n",
            "Epoch 7 Batch 370 Loss 0.2860\n",
            "Epoch 7 Loss 0.2928\n",
            "Time taken for 1 epoch 1785.2928128242493 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1700\n",
            "Epoch 8 Batch 10 Loss 0.2067\n",
            "Epoch 8 Batch 20 Loss 0.1691\n",
            "Epoch 8 Batch 30 Loss 0.1580\n",
            "Epoch 8 Batch 40 Loss 0.2200\n",
            "Epoch 8 Batch 50 Loss 0.2060\n",
            "Epoch 8 Batch 60 Loss 0.2474\n",
            "Epoch 8 Batch 70 Loss 0.2125\n",
            "Epoch 8 Batch 80 Loss 0.2178\n",
            "Epoch 8 Batch 90 Loss 0.2597\n",
            "Epoch 8 Batch 100 Loss 0.2184\n",
            "Epoch 8 Batch 110 Loss 0.2590\n",
            "Epoch 8 Batch 120 Loss 0.2281\n",
            "Epoch 8 Batch 130 Loss 0.1651\n",
            "Epoch 8 Batch 140 Loss 0.2332\n",
            "Epoch 8 Batch 150 Loss 0.2123\n",
            "Epoch 8 Batch 160 Loss 0.1521\n",
            "Epoch 8 Batch 170 Loss 0.2489\n",
            "Epoch 8 Batch 180 Loss 0.2091\n",
            "Epoch 8 Batch 190 Loss 0.2246\n",
            "Epoch 8 Batch 200 Loss 0.1718\n",
            "Epoch 8 Batch 210 Loss 0.2167\n",
            "Epoch 8 Batch 220 Loss 0.2356\n",
            "Epoch 8 Batch 230 Loss 0.1915\n",
            "Epoch 8 Batch 240 Loss 0.2590\n",
            "Epoch 8 Batch 250 Loss 0.2103\n",
            "Epoch 8 Batch 260 Loss 0.2246\n",
            "Epoch 8 Batch 270 Loss 0.2836\n",
            "Epoch 8 Batch 280 Loss 0.2021\n",
            "Epoch 8 Batch 290 Loss 0.2556\n",
            "Epoch 8 Batch 300 Loss 0.1867\n",
            "Epoch 8 Batch 310 Loss 0.2436\n",
            "Epoch 8 Batch 320 Loss 0.2351\n",
            "Epoch 8 Batch 330 Loss 0.2523\n",
            "Epoch 8 Batch 340 Loss 0.2946\n",
            "Epoch 8 Batch 350 Loss 0.2837\n",
            "Epoch 8 Batch 360 Loss 0.2471\n",
            "Epoch 8 Batch 370 Loss 0.2233\n",
            "Epoch 8 Loss 0.2159\n",
            "Time taken for 1 epoch 1778.3933482170105 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1713\n",
            "Epoch 9 Batch 10 Loss 0.1440\n",
            "Epoch 9 Batch 20 Loss 0.1683\n",
            "Epoch 9 Batch 30 Loss 0.1785\n",
            "Epoch 9 Batch 40 Loss 0.1385\n",
            "Epoch 9 Batch 50 Loss 0.1556\n",
            "Epoch 9 Batch 60 Loss 0.1826\n",
            "Epoch 9 Batch 70 Loss 0.0945\n",
            "Epoch 9 Batch 80 Loss 0.1517\n",
            "Epoch 9 Batch 90 Loss 0.1353\n",
            "Epoch 9 Batch 100 Loss 0.1622\n",
            "Epoch 9 Batch 110 Loss 0.1366\n",
            "Epoch 9 Batch 120 Loss 0.1686\n",
            "Epoch 9 Batch 130 Loss 0.1606\n",
            "Epoch 9 Batch 140 Loss 0.2189\n",
            "Epoch 9 Batch 150 Loss 0.1281\n",
            "Epoch 9 Batch 160 Loss 0.1550\n",
            "Epoch 9 Batch 170 Loss 0.1733\n",
            "Epoch 9 Batch 180 Loss 0.1896\n",
            "Epoch 9 Batch 190 Loss 0.1785\n",
            "Epoch 9 Batch 200 Loss 0.1323\n",
            "Epoch 9 Batch 210 Loss 0.1697\n",
            "Epoch 9 Batch 220 Loss 0.2436\n",
            "Epoch 9 Batch 230 Loss 0.1672\n",
            "Epoch 9 Batch 240 Loss 0.1523\n",
            "Epoch 9 Batch 250 Loss 0.2441\n",
            "Epoch 9 Batch 260 Loss 0.1897\n",
            "Epoch 9 Batch 270 Loss 0.1849\n",
            "Epoch 9 Batch 280 Loss 0.1643\n",
            "Epoch 9 Batch 290 Loss 0.1779\n",
            "Epoch 9 Batch 300 Loss 0.1403\n",
            "Epoch 9 Batch 310 Loss 0.1702\n",
            "Epoch 9 Batch 320 Loss 0.2029\n",
            "Epoch 9 Batch 330 Loss 0.2259\n",
            "Epoch 9 Batch 340 Loss 0.1933\n",
            "Epoch 9 Batch 350 Loss 0.1496\n",
            "Epoch 9 Batch 360 Loss 0.1908\n",
            "Epoch 9 Batch 370 Loss 0.2483\n",
            "Epoch 9 Loss 0.1668\n",
            "Time taken for 1 epoch 1767.0076427459717 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1443\n",
            "Epoch 10 Batch 10 Loss 0.1385\n",
            "Epoch 10 Batch 20 Loss 0.1159\n",
            "Epoch 10 Batch 30 Loss 0.1131\n",
            "Epoch 10 Batch 40 Loss 0.1041\n",
            "Epoch 10 Batch 50 Loss 0.1125\n",
            "Epoch 10 Batch 60 Loss 0.1249\n",
            "Epoch 10 Batch 70 Loss 0.1430\n",
            "Epoch 10 Batch 80 Loss 0.1239\n",
            "Epoch 10 Batch 90 Loss 0.1153\n",
            "Epoch 10 Batch 100 Loss 0.1347\n",
            "Epoch 10 Batch 110 Loss 0.1303\n",
            "Epoch 10 Batch 120 Loss 0.1914\n",
            "Epoch 10 Batch 130 Loss 0.1128\n",
            "Epoch 10 Batch 140 Loss 0.1070\n",
            "Epoch 10 Batch 150 Loss 0.1061\n",
            "Epoch 10 Batch 160 Loss 0.1300\n",
            "Epoch 10 Batch 170 Loss 0.1437\n",
            "Epoch 10 Batch 180 Loss 0.1170\n",
            "Epoch 10 Batch 190 Loss 0.1244\n",
            "Epoch 10 Batch 200 Loss 0.1129\n",
            "Epoch 10 Batch 210 Loss 0.1562\n",
            "Epoch 10 Batch 220 Loss 0.1251\n",
            "Epoch 10 Batch 230 Loss 0.1265\n",
            "Epoch 10 Batch 240 Loss 0.1411\n",
            "Epoch 10 Batch 250 Loss 0.1148\n",
            "Epoch 10 Batch 260 Loss 0.1521\n",
            "Epoch 10 Batch 270 Loss 0.1199\n",
            "Epoch 10 Batch 280 Loss 0.1161\n",
            "Epoch 10 Batch 290 Loss 0.1647\n",
            "Epoch 10 Batch 300 Loss 0.1616\n",
            "Epoch 10 Batch 310 Loss 0.1367\n",
            "Epoch 10 Batch 320 Loss 0.1321\n",
            "Epoch 10 Batch 330 Loss 0.1550\n",
            "Epoch 10 Batch 340 Loss 0.1591\n",
            "Epoch 10 Batch 350 Loss 0.1802\n",
            "Epoch 10 Batch 360 Loss 0.1058\n",
            "Epoch 10 Batch 370 Loss 0.1572\n",
            "Epoch 10 Loss 0.1323\n",
            "Time taken for 1 epoch 1742.6192276477814 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTAsuWhhZgkL"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNOW5fmhZi74"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGwdayaOZm3P"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEG2ca7dZnbN"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vydp0mfBZqI5"
      },
      "source": [
        "translate(u'hace mucho frio aqui.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}